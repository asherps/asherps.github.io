<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>diverse preference learning for capabilities and alignment - Personal Website</title>
  <link rel="stylesheet" href="../css/style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <style>body { background: white; }</style>
</head>
<body>
  <nav class="top-nav">
    <a href="../index.html">Home</a>
    <a href="../about.html">About</a>
    <a href="../blog.html">Blog</a>
    <a href="../updates.html">Updates</a>
  </nav>
  <div class="container">
    <aside class="sidebar">
      <a href="../index.html">
        <img src="../images/profile.jpg" alt="Profile Photo" class="profile-photo">
      </a>
      <nav class="social">
        <a href="https://twitter.com/asher5772" target="_blank"><i class="fab fa-twitter"></i></a>
        <a href="https://scholar.google.com/citations?user=rJMoYZEAAAAJ&hl=en" target="_blank"><i class="fas fa-graduation-cap"></i></a>
        <a href="https://instagram.com/asher_bps" target="_blank"><i class="fab fa-instagram"></i></a>
        <a href="https://www.linkedin.com/in/asher-parker-sartori-456033245/" target="_blank"><i class="fab fa-linkedin"></i></a>
        <a href="https://www.lesswrong.com/users/fiyr" target="_blank" class="lw-link"><span class="lw-icon">lw</span></a>
        <a href="https://www.youtube.com/channel/UCBmUpx94wuIksVvuy1fR2LQ" target="_blank"><i class="fab fa-youtube"></i></a>
      </nav>
    </aside>
    <main class="content blog-post-content" style="margin-left: 200px;">
      <article>
        <h1>diverse preference learning for capabilities and alignment</h1>
        <div class="post-meta">
          <span class="date">April 12, 2025</span>
        </div>
        <div class="post-content">
          <p>New paper with Stewart Slocum and Dylan Hadfield-Menell is out! We find that alignment algorithms like RLHF and DPO significantly reduce the diversity of LLM outputs during alignment posttraining, and that the KL divergence regularizer causes models to systematically overweight majority opinions and sacrifice diversity. (<a href="https://arxiv.org/abs/2511.08594">paper</a>)</p>
          <br>
          <p>We introduce Soft Preference Learning, which decouples entropy and cross-entropy components in the KL penalty to enable finer control over output diversity. Models trained with this approach achieve higher accuracy on difficult repeated sampling tasks and demonstrate greater semantic and lexical diversity. On the alignment side, the trained models can represent a wider range of societal viewpoints and show improved logit calibration.</p>
          <br>
          <p>Accepted to ICLR 2025.</p>
        </div>
      </article>
    </main>
  </div>
  <script src="../js/error-handler.js"></script>
</body>
</html>
