<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>rule-following might scale - Quick Take</title>
  <link rel="stylesheet" href="../css/style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <style>body { background: white; }</style>
</head>
<body>
  <nav class="top-nav">
    <a href="../index.html">Home</a>
    <a href="../about.html">About</a>
    <a href="../blog.html">Blog</a>
    <a href="../updates.html">Updates</a>
  </nav>
  <div class="container">
    <aside class="sidebar">
      <a href="../index.html">
        <img src="../images/profile.jpg" alt="Profile Photo" class="profile-photo">
      </a>
      <nav class="social">
        <a href="https://twitter.com/asher5772" target="_blank"><i class="fab fa-twitter"></i></a>
        <a href="https://scholar.google.com/citations?user=rJMoYZEAAAAJ&hl=en" target="_blank"><i class="fas fa-graduation-cap"></i></a>
        <a href="https://instagram.com/asher_bps" target="_blank"><i class="fab fa-instagram"></i></a>
        <a href="https://www.linkedin.com/in/asher-parker-sartori-456033245/" target="_blank"><i class="fab fa-linkedin"></i></a>
        <a href="https://www.lesswrong.com/users/fiyr" target="_blank" class="lw-link"><span class="lw-icon">lw</span></a>
        <a href="https://www.youtube.com/channel/UCBmUpx94wuIksVvuy1fR2LQ" target="_blank"><i class="fab fa-youtube"></i></a>
      </nav>
    </aside>
    <main class="content blog-post-content" style="margin-left: 200px;">
      <article>
        <h1>rule-following might scale</h1>
        <div class="post-meta">
          <span class="date">May 5, 2025</span>
        </div>
        <div class="post-content">
          <p><em>*Note: Most quick takes on this website were copied over from somewhere else and adapted by Claude.</em></p>
          <br>
          <p>I think rule-following may be fairly natural for minds to learn, much more so than other safety properties, and I think it might be worth more research in this direction.</p>
          <br>
          <p>Some pieces of weak evidence:</p>
          <p>1. Most humans seem to learn and follow broad rules, even when they are capable of more detailed reasoning.</p>
          <p>2. Current LLMs seem to follow rules fairly well (and training often incentivizes them to learn broad heuristics rather than detailed, reflective reasoning).</p>
          <br>
          <p>While I do expect rule-following to become harder to instill as AIs become smarter, I think that if we are sufficiently careful, it may well scale to human-level AGIs. I think trying to align reflective, utilitarian-style AIs is probably really hard, as these agents are much more prone to small unavoidable alignment failures (like slight misspecification or misgeneralization) causing large shifts in behavior. Conversely, if we try our best to instill specific simpler rules, and then train these rules to take precedence over consequentialist reasoning whenever possible, this seems a lot safer.</p>
          <br>
          <p>I also think there is a bunch of tractable, empirical research that we can do right now about how to best do this.</p>
        </div>
      </article>
    </main>
  </div>
  <script src="../js/error-handler.js"></script>
</body>
</html>
